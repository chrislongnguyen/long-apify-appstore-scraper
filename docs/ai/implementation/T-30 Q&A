1) How do we ensure Reddit data is directly relevant?
**Mechanism (I1):** We rely on Search Query Design + Reddit sort="relevance" + LLM triangulation. Strict inputs (subreddit + search-query URLs) and Reddit's relevance sort pre-filter; the LLM downweights off-topic lines. Optional later: keyword-density pre-filter before _summarize_reddit() to reduce token dilution or support explicit Reddit citations.
Current state (pre-filter): there is no Python-side relevance scoring. Every post the Apify actor returns gets normalized and passed through. The only "filtering" is:
URL-level scoping (fetcher_reddit.py ~72–88): we build subreddit-scoped search URLs like reddit.com/r/nosurf/search?q=screen+time+app&restrict_sr=1. This helps, but the actor still returns whatever Reddit's search ranking gives us.
Truncation (_summarize_reddit ~213): only first 25 items' titles and text are summarized; the rest are dropped silently.
What's missing:
No keyword/relevance scoring. A post titled "Share Your Screen Time" from r/TwentiesIndia and a post about "New Day, New Rapido Scam" from r/pune both made it into your reddit_context.json — clearly irrelevant noise. There is no step that scores each Reddit post for relevance to the niche or app before feeding it to the LLM.
No deduplication by content. Duplicates across global search and scoped search can appear.
No comment inclusion. As we discussed, the 1300 comments are stored in the cache but never summarized for the LLM.
What would improve this:
A relevance scorer (could be keyword overlap with pain_keywords, or a simple TF-IDF/cosine similarity against the app name and niche keywords) that filters or ranks Reddit items before summarization.
Comment sampling: surface the top 2–3 highest-signal comments per post (e.g., longest, or matching domain vocabulary) into the summary.
Dedup by URL before normalization.

2) How do Reddit + Apple reviews combine into the System Map?
They combine, but only indirectly, and Reddit drops out after Stage 1.
Here's the exact data flow across the 3 stages:
Stage	What receives Reddit?	What receives App Store?
Stage 1 (ICP)	Yes — reddit_summary string under REDDIT/CONTEXT: in the user prompt	Yes — pain_quotes (1-2★) and success_quotes (5★ whales)
Stage 2 (System Map)	No — only receives icp JSON (output of Stage 1) + curated_evidence (App Store reviews only) + metrics	Yes — _curate_evidence(pain_reviews, success_reviews) pulls directly from App Store reviews
Stage 3 (EPS)	No — only receives system_map JSON + icp JSON	No — only receives Stage 1+2 outputs
So the pipeline looks like:
Reddit ──→ Stage 1 (ICP) ──→ icp JSON                                  ↓App Store reviews ──→ Stage 1     ↓App Store reviews ──→ Stage 2 (System Map) ← icp JSON                                  ↓                     Stage 3 (EPS) ← system_map + icp                                  ↓                     Blueprint + system_map.json
The critical gap: Stage 2 (map_system_dynamics) constructs the 7 nodes (UDO, UDS, UBS, etc.) using:
The ICP JSON (which was influenced by Reddit in Stage 1)
curated_evidence — but this is only App Store reviews (see line 457: evidence = self._curate_evidence(pain_reviews, success_reviews)). Reddit posts/comments are not in this evidence pool.
So in your opal_screen_time_system_map.json, the evidence quotes on each node (UDS, UBS, etc.) are all from App Store reviews. Reddit's influence is only implicit — it shaped the ICP's alternatives, when_trigger, and why_udo, which the LLM then uses as context when populating nodes.
What would strengthen this: Pass a subset of relevant Reddit quotes directly into the Stage 2 evidence pool (alongside App Store reviews), so the LLM can cite Reddit posts as evidence for nodes. The prompt already says "evidence from reviews or Reddit" (line 72) but the code doesn't supply Reddit evidence to Stage 2.

3) Data science practices audit: collection, cleaning, signal quality
Let me assess each layer:
Collection — Good
App Store: Apify actor with retry logic (tenacity), date filtering (90 days), configurable max_reviews, smoke test mode. Solid.
Reddit: URL-scoped fetching with subreddit + search query combinations. Reasonable but no dedup or relevance gate.
Cleaning — Good for App Store, Weak for Reddit
App Store:
Date filter (90 days, timezone-aware) ✅
5-star drop before analysis (reduces happy-path noise) ✅
Whale detector (3× weight for >40 words or domain vocab) ✅
Pain keyword dictionary (10 categories, weighted) ✅
Reddit:
No relevance filtering ❌
No dedup ❌
Comments discarded ❌
Many items arrive with empty text, subreddit, comments — normalization doesn't recover these ❌
Organization — Good
Niche subdirectories (data/{niche}/, reports/{niche}/) ✅
Separate cache files: *_reviews.json, *_analysis.json, reddit_context.json, *_system_map.json ✅
Config-driven (targets.json, settings.json, pain_keywords.json) ✅
Signal Processing — Mixed
Strengths:
MECE risk scoring with severity-first model (T-031) ✅
Whale weighting in keyword density and churn ✅
Forensic intelligence (timeline, n-grams, migration, matrix) ✅
Fermi estimator with niche-category multipliers ✅
Slope delta for trend acceleration ✅
Gaps:
Reddit signal is unfiltered noise → LLM prompt includes irrelevant posts
No cross-validation between Reddit themes and App Store pain categories
_summarize_reddit is a simple title dump — no thematic clustering or frequency analysis
Stage 2 evidence pool excludes Reddit entirely
No sentiment scoring on Reddit posts before inclusion
Overall verdict
App Store pipeline is production-quality for a project at this stage. Reddit pipeline is alpha — it fetches and caches data but doesn't clean, score, or integrate it deeply enough to guarantee the signal reaches the reports meaningfully.

4) User Personas and User Stories in the Blueprint
Currently: No. The blueprint template (venture_blueprint.j2) does not render user personas or user stories. It jumps straight to "The System Map" (UDO, driving/blocking forces, incumbent failure).
The ICP data that could power personas is already in system_map.json → icp:
who.demographic / who.psychographic
why_udo
icp_segment.primary / secondary / whale_segment
pain_success_paradox
when_trigger
alternatives
But right now icp_segment is identical across all three segments (primary = secondary = whale — the LLM collapsed them into one). And there is no prompt instruction asking for 3 non-overlapping personas with user stories.
What you'd need:
Schema change: Add a user_personas: List[UserPersona] field to HolographicICP (or a new Stage 0.5 output) with fields like persona_name, archetype, user_story (the 5-6 sentence journey: User → Verb → Desired Outcome → UBS → UDS → etc.), segment (primary/secondary/whale).
Prompt change: Add explicit instructions to the ICP prompt: "Identify 3 non-overlapping user personas. For each, write a 5-6 sentence User Story that traces their journey through: Who they are → What they want (UDO) → What drives them (UDS/UDS.UD) → What blocks them (UBS/UBS.UD) → Why they fail (UDS.UB) → What could save them (UBS.UB)."
Template change: Add a "## 0. The People" section at the top of venture_blueprint.j2 that renders each persona and their story before the System Map.
This is a great idea — it would make the blueprint immediately actionable for founders and investors because they could see "who are we building for?" before diving into the system dynamics. The data to support it is already being collected; it just needs a prompt + schema + template change.
TL;DR
#	Question	Status
1	Reddit relevance	No relevance filtering — noise gets through. Need a scorer/filter.
2	Reddit + App Store in System Map	Reddit only enters Stage 1 (ICP). Stage 2 evidence is App Store only. Reddit doesn't populate UDS/UBS nodes directly.
3	Data science practices	App Store pipeline is strong. Reddit pipeline is alpha (no cleaning, no dedup, comments dropped, no relevance gate).
4	User Personas in Blueprint	Not implemented. ICP data exists but schema/prompt/template don't produce 3 distinct personas with user stories.